# --- AI Router: Coolify / Production Environment Variables ---

# 1. AI Router Configuration
# API Key for access (Required)
AI_ROUTER_API_KEY=

# Environment Mode
AI_ROUTER_ENV=production

# 2. Database (Postgres)
# Connection string: postgresql://user:pass@host:5432/dbname
POSTGRES_URL=postgresql://postgres:password@postgres:5432/ai_router

# 3. Redis (Queue & Caching)
# Connection string: redis://host:6379/0
REDIS_URL=redis://redis:6379/0

# 4. Ollama (GPU Local Inference)
# Internal Docker URL for Ollama service
OLLAMA_BASE_URL=http://ollama:11434
# Default model to pull/use
OLLAMA_MODEL=deepseek-coder:6.7b

# 5. GPU Queue Control
# Max concurrent requests to send to GPU (Ollama)
# Set to 1 or 2 to avoid VRAM OOM or excessive latency on consumer cards
GPU_QUEUE_MAX_WORKERS=1
# Timeout (seconds) for request in queue before dropping
GPU_QUEUE_TIMEOUT=60

# 6. Cloud Fallback (Optional)
# Set to 1 to enable OpenAI fallback, 0 to force local-only
ENABLE_OPENAI_FALLBACK=0
OPENAI_API_KEY=

# 7. Cloudflare Tunnel (Optional, for Public Access)
# Token from Cloudflare Zero Trust Dashboard
CLOUDFLARE_TUNNEL_TOKEN=
